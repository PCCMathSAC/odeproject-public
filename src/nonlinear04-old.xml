<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the book                            	   -->
<!--                                                          	   -->
<!--    Ordinary Differential Equations Project      		  	   -->
<!--                                                         	   -->
<!-- Copyright (C) 2013-2017 Thomas W. Judson  					   -->
<!-- See the file COPYING for copying conditions.             	   -->

<section xml:id="nonlinear04" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Lyapunov Functions and Gradient Systems</title>
    
    <subsection>
    	<title>An Example</title>

        <p>It is easy to see that the system
			<mdn>
				<mrow xml:id="equation-nonlinear04-lyapunov-1">\frac{dx}{dt} &amp; = y + \alpha x(x^2 + y^2)</mrow>
				<mrow xml:id="equation-nonlinear04-lyapunov-2">\frac{dy}{dt} &amp; = -x + \alpha y(x^2 + y^2)</mrow>
			</mdn>
		has an equilibrium solution at the origin no matter what the value of <m>\alpha</m> is.  The Jacobian of this system is
			<me>J
			=
			\begin{pmatrix}
			3 \alpha x^2 + \alpha y^2 &amp; 1 + 2 \alpha xy \\
			-1 + 2 \alpha x y &amp; \alpha x^2 + 3 \alpha y^2
			\end{pmatrix}.</me>
		Since our equilibrium solution is the origin, 
			<me>J =
			\begin{pmatrix}
			0 &amp; 1 \\
			-1 &amp; 0
			\end{pmatrix}</me>
		and the linearization of our system at the origin is
			<md>
				<mrow>\frac{dx}{dt} &amp; = y</mrow>
				<mrow>\frac{dy}{dt} &amp; = -x.</mrow>
			</md>
		Since the eigenvalues of 
			<me>J =
			\begin{pmatrix}
			0 &amp; 1 \\
			-1 &amp; 0
			\end{pmatrix},</me>
		are <m>\pm i</m>, the linearization has a center at the origin.  The phase plane consists of circles about the origin (Figure<nbsp /><xref ref="figure-nonlinear04-linearization-solution-curves-1" />).  Notice that the linearization does not depend on <m>\alpha</m>.</p>


		<figure xml:id="figure-nonlinear04-linearization-solution-curves-1">
            <caption>Solution curves for the linearization.</caption>
            <image width="350" source="images/linearization-solution-curves-1.png" />
        </figure>

		<p>Now let us consider what happens to system<nbsp /><xref ref="equation-nonlinear04-lyapunov-1" /><ndash /><xref ref="equation-nonlinear04-lyapunov-2" /> if we consider different values of <m>\alpha</m>.  If <m>\alpha = 5</m>, the situation is quite different that the linearization of our system.  A solution curve spirals out from the origin as <m>t \to \infty</m> (Figure<nbsp /><xref ref="figure-nonlinear04-linearization-solution-curves-2" />).  As <m>t \to - \infty</m>, the solution curve spirals back into the origin, but it seems to stop before actually reaching the origin.  If <m>\alpha = -5</m> on the other hand, we seem to have the opposite behavior with the solution curves spiraling into the origin as <m>t \to \infty</m>.  As before, the solutions do not seem to reach the origin (Figure<nbsp /><xref ref="figure-nonlinear04-linearization-solution-curves-3" />).</p>

		<figure xml:id="figure-nonlinear04-linearization-solution-curves-2">
            <caption>Solution curves <m>\alpha = 5</m>.</caption>
            <image width="350" source="images/linearization-solution-curves-2.png" />
        </figure>

		<figure xml:id="figure-nonlinear04-linearization-solution-curves-3">
            <caption>Solution curves for <m>\alpha = -5</m>.</caption>
            <image width="350" source="images/linearization-solution-curves-3.png" />
        </figure>

		<p>Suppose that <m>(x(t), y(t))</m> is a solution to the nonlinear system.  The function
			<me>r(t) = \sqrt{x(t)^2 + y(t)^2}</me>
		is the distance of a point on the solution curve  to the origin in the <m>xy</m>-plane.  To see how <m>r</m> changes as <m>t \to \pm \infty</m>, we can compute the derivative of <m>r</m>.  Actually, it is easier to work with the equation <m>r(t)^2 = x(t)^2 + y(t)^2</m>.  Thus,
			<md>
				<mrow>\frac{d}{dt} r^2 &amp; = \frac{d}{dt} (x(t)^2 + y(t)^2)</mrow>
				<mrow>&amp; = 2 x \frac{dx}{dt} + 2y \frac{dy}{dt}</mrow>
				<mrow>&amp; = 2x (y + \alpha x(x^2 + y^2) ) + 2y( -x + \alpha y(x^2 + y^2))</mrow>
				<mrow>&amp; = 2 \alpha (x^2 + y^2)^2</mrow>
				<mrow>&amp; = 2 \alpha r^4.</mrow>
			</md>
		Since <m>(r^2)' = 2r r'</m>, we have 
			<men xml:id="equation-nonlinear04-lyapunov-3">r' = \alpha r^3</men>
		for <m>r \neq 0</m>.</p>

		<p>Equation<nbsp /><xref ref="equation-nonlinear04-lyapunov-3" /> is separable, and it is quite easy to determine the solution as
			<me>r(t) = \frac{1}{\sqrt{C - 2 \alpha t}}.</me>
		However, we do not need to know this solution  to determine the nature of the equilibrium solution at the origin.  If <m>\alpha \gt 0</m> and <m>t \to -\infty</m>, equation<nbsp /><xref ref="equation-nonlinear04-lyapunov-3" /> tells us that <m>r(t) \to 0</m>.  Thus, any solution to the system<nbsp /><xref ref="equation-nonlinear04-lyapunov-1" /><ndash /><xref ref="equation-nonlinear04-lyapunov-2" /> we have a spiral sink at the origin if <m>\alpha = -5</m>.  Even though linearization fails to tell us the nature of the equilibrium solution at the origin, we were able to determine the nature of the equilibrium solution with further analysis.</p>
			
	</subsection>

    <subsection>
    	<title>Lyapunov Functions</title>

		<p>We will now try to exploit what we have learned from our last example and from Hamiltonian systems to see if it is possible to analyze more general systems.  If we consider solutions, <m>(x(t), y(t))</m>, of the system
			<md>
				<mrow>\frac{dx}{dt} &amp; = f(x, y)</mrow>
				<mrow>\frac{dy}{dt} &amp; = g(x, y),</mrow>
			</md>
		we might ask how a function <m>V(x, y)</m> varies along the solution curve.  We already have an answer if our system is Hamiltonian, and <m>V</m> is the corresponding Hamiltonian function.  In this case <m>dV/dt = 0</m>.  In general, we  know that
			<md>
				<mrow>\frac{dV}{dt} (x(t), y(t)) &amp; = \frac{\partial V}{\partial x} \frac{dx}{dt} + \frac{\partial V}{\partial y} \frac{dy}{dt}</mrow>
				<mrow>&amp; =\frac{\partial V}{\partial x} f(x, y) + \frac{\partial V}{\partial y} g(x, y).</mrow>
			</md>
		Thus, if we let
			<me>\dot{V} =\frac{\partial V}{\partial x} f(x, y) + \frac{\partial V}{\partial y} g(x, y),</me>
		we know that
			<me>\frac{dV}{dt}(x(t), y(t)) = \dot{V} (x(t), y(t)).</me>
		Thus, <m>V</m> is increasing along a solution curve if <m>\dot{V}(x, y) \gt 0</m> and decreasing along a solution curve if <m>\dot{V}(x, y) \lt 0</m>. Our example suggests that we can determine this information without finding the solution.</p>

		<p>Recall that the gradient of a function <m>V: {\mathbb R}^2 \rightarrow {\mathbb R}</m> is 
			<me>\nabla V = \left(\frac{\partial V}{\partial x}, \frac{\partial V}{\partial y}\right).</me>
		If 
			<me>{\mathbf F}(x, y) =
			\begin{pmatrix}
			f(x, y) \\ g(x, y)
			\end{pmatrix},</me>
		then
			<me>\dot{V} = \nabla V \cdot {\mathbf F},</me>
		is the directional derivative of <m>V</m> in the direction of <m>{\mathbf F}</m>.</p>

		<p>Let us use this new information about <m>V</m> to obtain information about equilibrium solutions of our system.  We do know that <m>V(x, y)</m> graphs as a surface in <m>{\mathbb R}^3</m> and 
			<me>V(x, y) = \text{constant}</me>
		gives the contour lines or level curves of the surface in the <m>xy</m>-plane.<fn>See Figures 1 and 2 in John Polking, Albert Boggess, and David Arnold.  {\it Differential Equations}.  Prentice Hall, Upper Saddle River, NJ, 2001, p. 611.</fn>  We also know that the gradient of <m>V</m> points in the direction that <m>V</m> is increasing the fastest and that the gradient is orthogonal to the level curves of <m>V</m>.  Thus, if <m>\dot{V}(x, y) \gt 0</m>, we know that <m>V</m> is increasing in the direction of the direction of the vector field <m>{\mathbf F}</m> and the elevation of the solution curve through <m>(x, y)</m> in <m>{\mathbb R}^3</m> is increasing.  That is, the solution curve is traveling uphill.  Similarly, if <m>\dot{V}(x, y) \lt 0</m>, we know that the solution curve at <m>(x, y)</m> is going downhill.<fn>The argument that we have made here also works in higher dimensions.</fn></p>

		<p>Now suppose that <m>V</m> is a real-valued function defined on a set <m>S</m> in the <m>xy</m>-plane, where the point <m>{\mathbf x}_0 = (x_0, y_0)</m> is in <m>S</m>.  We say that <m>V</m> is <term>positive definite</term> if <m>V({\mathbf x}) \gt 0</m> for all <m>{\mathbf x}</m> in <m>S</m>, where <m>{\mathbf x} \neq {\mathbf x}_0</m>, and <m>V</m> is <term>positive semidefinite</term> if <m>V({\mathbf x}) \geq 0</m>.  Similarly, we say that <m>V</m> is <term>negative definite</term> and <term>negative semidefinite</term> if <m>V({\mathbf x}) \lt 0</m> and <m>V({\mathbf x}) \leq 0</m>, respectively.</p>

		<p>For example, if we consider the system for a harmonic oscillator
			<md>
				<mrow>y' &amp; = v</mrow>
				<mrow>v' &amp; = -\frac{k}{m} y - \frac{b}{m} v,</mrow>
			</md>
		then the energy function, 
			<me>E(y, v) = \frac{1}{2} mv^2 + \frac{1}{2} ky^2,</me>
		is positive definite.</p>

		<theorem xml:id="theorem-nonlinear04-lyapunov">
            <statement>
                <p>Suppose that the system
					<md>
						<mrow>\frac{dx}{dt} &amp; = f(x, y)</mrow>
						<mrow>\frac{dy}{dt} &amp; = g(x, y)</mrow>
					</md>
				has an equilibrium solution at <m>(x_0, y_0)</m>.  Let <m>V</m> be a continuously differentiable function defined on a neighborhood <m>U</m> of <m>(x_0, y_0)</m> that is positive definite with minimum at <m>(x_0, y_0)</m>.
					<ol>

						<li>If <m>\dot{V}</m> is negative semidefinite on <m>U</m>, then <m>(x_0, y_0)</m> is a stable equilibrium solution.  That is, any solution that starts near the equilibrium solution will stay near the equilibrium solution.</li>

						<li>If <m>\dot{V}</m> is negative definite on <m>U</m>, then <m>(x_0, y_0)</m> is a asymptotically stable equilibrium solution or a sink.</li>

					</ol></p>
            </statement>
        </theorem> 

		<p>Recall our example,
			<md>
				<mrow>\frac{dx}{dt} &amp; = y + \alpha x(x^2 + y^2)</mrow>
				<mrow>\frac{dy}{dt} &amp; = -x + \alpha y(x^2 + y^2).</mrow>
			</md>
		The function <m>V(x, y) = x^2 + y^2</m> is positive definite on <m>{\mathbb R}^2</m>, with an isolated minimum at the origin.  We can compute
			<md>
				<mrow>\dot{V}&amp; = \frac{\partial V}{\partial x}(x, y)  f(x, y) + \frac{\partial V}{\partial y}(x, y) g(x, y)</mrow>
				<mrow>&amp; = 2x (y + \alpha x(x^2 + y^2) ) + 2y( -x + \alpha y(x^2 + y^2))</mrow>
				<mrow>&amp; = 2 \alpha (x^2 + y^2)^2.</mrow>
			</md>
		If <m>\alpha \lt 0</m>, then <m>\dot{V}</m> is negative definite on <m>{\mathbb R}^2</m>.  Theorem<nbsp /><xref ref="theorem-nonlinear04-lyapunov" /> tells us that the origin is a stable equilibrium point.</p>

		<p>The function <m>V</m> in Theorem<nbsp /><xref ref="theorem-nonlinear04-lyapunov" /> is called a <term>Lyapunov function</term>.  If we compare Theorem 1 to using linearization to determine stability of an equilibrium solution, we will find that we can apply this result where linearization fails.  Also, Lyapunov functions are defined on a domain <m>U</m>, where linearization only tells us what happens on a small neighborhood around the equilibrium solution.  Unfortunately, there are no general ways of finding Lyapunov functions.</p>

	</subsection>

    <subsection>
    	<title>Gradient Systems</title>

		<p>Let <m>S</m> be a real-valued function on the <m>xy</m>-plane. The gradient of <m>S</m> is
			<me>\nabla S = \left(  \frac{\partial S}{\partial x},  \frac{\partial S}{\partial y} \right).</me>
		The system
			<md>
				<mrow>\frac{dx}{dt} &amp; = f(x, y)</mrow>
				<mrow>\frac{dy}{dt} &amp; = g(x, y)</mrow>
			</md>
		is a <term>gradient system</term> if 
			<md>
				<mrow>f(x, y) &amp; = \frac{\partial S}{\partial x}</mrow>
				<mrow>g(x, y) &amp; = \frac{\partial S}{\partial y}.</mrow>
			</md>
		For example, the system
			<md>
				<mrow>\frac{dx}{dt} &amp; = x - x^3</mrow>
				<mrow>\frac{dy}{dt} &amp; = -y</mrow>
			</md>
		is a gradient system, where
			<me>S(x, y) = \frac{x^2}{2} - \frac{x^4}{4} - \frac{y^2}{2} + 8.</me>
		Now let us see what happens on the solution curves of this gradient system.  If <m>(x(t), y(t))</m> is a solution curve, 
			<me>\frac{dS}{dt} = \frac{\partial S}{\partial x} \frac{dx}{dt} + \frac{\partial S}{\partial y} \frac{dy}{dt} = (x - x^3)^2 + y^2 \geq 0.</me>
		Thus, <m>S</m> increases at point on the solution curve where the gradient of <m>S</m> is nonzero.  That is, <m>S</m> increases at every point on the solution curves except at the equilibrium points.</p>

		<p>In general, suppose that 
			<md>
				<mrow>\frac{dx}{dt} &amp; =  \frac{\partial S}{\partial x}</mrow>
				<mrow>\frac{dy}{dt} &amp; =  \frac{\partial S}{\partial y}</mrow>
			</md>
		is a gradient system.  Since
			<me>\frac{dS}{dt} (x(t), y(t)) = \frac{\partial S}{\partial x} \frac{dx}{dt}  + \frac{\partial S}{\partial y} \frac{dy}{dt} = \left(  \frac{\partial S}{\partial x} \right)^2 +  \left(  \frac{\partial S}{\partial y} \right)^2 \geq 0,</me>
		we know that <m>S</m> increases on every solution to the system except at the critical points of <m>S(x(t), y(t))</m>.</p> 

		<p>Let us see what this means in terms of the linearization of the system. The Jacobian matrix of <m>(f, g)</m> is
			<me>J =
			\begin{pmatrix}
			f_x &amp; f_y \\
			g_x &amp; g_y
			\end{pmatrix}
			=
			\begin{pmatrix}
			S_{xx} &amp; S_{xy} \\
			S_{yx} &amp; S_{yy}
			\end{pmatrix}.</me>
		Since <m>S_{xy} = S_{yx}</m>, the matrix <m>J</m> must have the form
			<me>J=
			\begin{pmatrix}
			\alpha &amp; \beta \\
			\beta &amp; \gamma
			\end{pmatrix},</me>
		where <m>\alpha = S_{xx}</m>, <m>\beta = S_{xy}</m>, and <m>\gamma = S_{yy}</m>.  The characteristic polynomial of <m>J</m> is 
			<me>\lambda^2 - (\alpha + \gamma) \lambda + \alpha \gamma - \beta^2.</me>
		Therefore, the eigenvalues are 
			<me>\lambda = \frac{1}{2}( \alpha + \beta) \pm \frac{1}{2} \sqrt{(\alpha - \gamma)^2 + 4 \beta^2}.</me>
		Since we have real eigenvalues, a gradient system can have no spiral sources, spiral sinks, or centers.</p>

		<p>For example, <m>x^2 + y^2</m> is a Lyapunov function for the system
			<md>
				<mrow>\frac{dx}{dt} &amp; =  -x + y</mrow>
				<mrow>\frac{dy}{dt} &amp; = -x - y.</mrow>
			</md>
		However, the origin is a spiral sink, so this system cannot be a gradient system.</p>

	</subsection>

    <subsection>
    	<title>Important Lessons</title>

		<p><ul>
		
			<li>Let <m>V</m> be a real-valued function defined on a set <m>S</m> in the <m>xy</m>-plane such that the point <m>{\mathbf x}_0 = (x_0, y_0)</m> is in <m>S</m> and <m>V({\mathbf x}_0) = 0</m>.
				<ul>

					<li>We say that <m>V</m> is <term>positive definite</term> if <m>V({\mathbf x}) \gt 0</m> for all <m>{\mathbf x}</m> in <m>S</m>, where <m>{\mathbf x} \neq {\mathbf x}_0</m>.</li>

					<li>We say that <m>V</m> is <term>positive semidefinite</term> if <m>V({\mathbf x}) \geq 0</m> for all <m>{\mathbf x}</m> in <m>S</m>.</li>

					<li>We say that <m>V</m> is <term>negative definite</term> if <m>V({\mathbf x}) \lt 0</m> for all <m>{\mathbf x}</m> in <m>S</m>, where <m>{\mathbf x} \neq {\mathbf x}_0</m>.</li>

					<li>We say that <m>V</m> is <term>negative semidefinite</term> if <m>V({\mathbf x}) \leq 0</m> for all <m>{\mathbf x}</m> in <m>S</m>.</li>

				</ul></li>

			<li>Suppose that the system
				<md>
					<mrow>\frac{dx}{dt} &amp; = f(x, y)</mrow>
					<mrow>\frac{dy}{dt} &amp; = g(x, y)</mrow>
				</md>
			has an equilibrium solution at <m>(x_0, y_0)</m>.  Let <m>V</m> be a continuously differentiable function defined on a neighborhood <m>U</m> of <m>(x_0, y_0)</m> that is positive definite with minimum at <m>(x_0, y_0)</m>.
				<ol>
				
					<li>If <m>\dot{V}</m> is negative semidefinite on <m>U</m>, then <m>(x_0, y_0)</m> is a stable equilibrium solution.  That is, any solution that starts near the equilibrium solution will stay near the equilibrium solution.</li>

					<li>If <m>\dot{V}</m> is negative definite on <m>U</m>, then <m>(x_0, y_0)</m> is a asymptotically stable equilibrium solution or a sink.</li>

				</ol>
			We can use these results to analyze the behavior of equilibrium solutions where linearization fails. The function <m>V</m> is called a <term>Lyapunov function</term>.  We have no general methods for finding Lyapunov functions.</li>

			<li>The system
				<md>
					<mrow>\frac{dx}{dt} &amp; = f(x, y)</mrow>
					<mrow>\frac{dy}{dt} &amp; = g(x, y)</mrow>
				</md>
			is a <term>gradient system</term> if 
				<md>
					<mrow>f(x, y) &amp; = \frac{\partial S}{\partial x}</mrow>
					<mrow>g(x, y) &amp; = \frac{\partial S}{\partial y},</mrow>
				</md>
			where <m>S</m> be a real-valued function on the <m>xy</m>-plane. Since
				<me>\frac{dS}{dt} (x(t), y(t)) = \left(  \frac{\partial S}{\partial x} \right)^2 + \left(  \frac{\partial S}{\partial y} \right)^2 \geq 0,</me>
			<m>S</m> increases on every solution to the system except at the critical points of <m>S(x(t), y(t))</m>. Since the eigenvalues of a gradient system are real, a gradient system has no spiral sources, spiral sinks, or centers.</li>

		</ul></p>
		
	</subsection>

    <xi:include href="./exercises/nonlinear04.xml" />

</section>


<!--</section>-->
